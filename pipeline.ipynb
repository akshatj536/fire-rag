{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "366b9989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq \"unstructured[all-docs]\"\n",
    "%pip install -Uq langchain_chroma\n",
    "%pip install -Uq langchain langchain-community langchain-openai\n",
    "%pip install -Uq python_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2d2b8b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "from langchain_core.documents import Document # has raw data + internal content\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma \n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf23118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a2d7ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_document(file_path):\n",
    "    \"\"\"\" Extract Documents from pdf using unstructured \"\"\"\n",
    "    elements = partition_pdf(\n",
    "        filename= file_path,\n",
    "        strategy= \"hi_res\",\n",
    "        infer_table_structure= True,\n",
    "        extract_image_block_types=[\"Image\"],\n",
    "        extract_image_block_to_payload=True\n",
    "    )\n",
    "    print(f\"Extracted {len(elements)} elements\")\n",
    "    return elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c4810b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "Extracted 137 elements\n"
     ]
    }
   ],
   "source": [
    "file_path = \"imagenet.pdf\"\n",
    "elements=partition_document(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "dcc359a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks_by_title(elements):\n",
    "    \"\"\"\" Create chunking by using title as the main differentiator\"\"\" \n",
    "\n",
    "    chunks = chunk_by_title(\n",
    "        elements,\n",
    "        combine_text_under_n_chars=500,\n",
    "        max_characters=3000,\n",
    "        new_after_n_chars=2400\n",
    "\n",
    "    )\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "40611e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 19 chunks\n"
     ]
    }
   ],
   "source": [
    "chunks=create_chunks_by_title(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fa46572a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_content_types(chunk):\n",
    "    ''' Analyze what types of content are there in a chunk''' \n",
    "    content_data = {\n",
    "        'text': chunk.text,\n",
    "        'tables': [],\n",
    "        'images': [],\n",
    "        'types' : ['text']\n",
    "    }\n",
    "    # check for tables and images in original elements\n",
    "    \n",
    "    for element in chunk.metadata.orig_elements:\n",
    "        element_type = type(element).__name__\n",
    "\n",
    "        if element_type == 'Table':\n",
    "            content_data['types'].append('table')\n",
    "            table_html = getattr(element.metadata,'text_as_html', \"table not found\")\n",
    "            #print(table_html)\n",
    "            content_data['tables'].append(table_html)\n",
    "            #print(content_data['tables'])\n",
    "        elif element_type == 'Image':\n",
    "            content_data['types'].append('image')\n",
    "            image_base64 = getattr(element.metadata,'image_base64', \"image not found\" )\n",
    "            #print(image_base64)\n",
    "            content_data['images'].append(image_base64)\n",
    "            #print(content_data['images'])\n",
    "    content_data['types']= list(set(content_data['types']))\n",
    "    return content_data\n",
    "\n",
    "def created_ai_summary(text:str, tables: List[str], images: List[str]) -> str:\n",
    "    \"\"\" Create AI enhanced summary \"\"\" \n",
    "    try:\n",
    "        llm=ChatOpenAI(model=\"gpt-4o\", temperature = 0)\n",
    "\n",
    "        prompt_text = f\"\"\"\n",
    "You are an expert content summarizer. Your job is to analyze the provided text, tables, and images \n",
    "and create a clear, concise, and structured summary.\n",
    "\n",
    "CONTENT TO ANALYZE:\n",
    "-------------------\n",
    "TEXT CONTENT:\n",
    "{text}\n",
    "\n",
    "\"\"\"\n",
    "        if tables:\n",
    "            prompt_text += \"TABLES:\\n\"\n",
    "            for i, table in enumerate(tables):\n",
    "                prompt_text += f\"Table {i+1}:\\n{table}\\n\\n\"\n",
    "\n",
    "        if images:\n",
    "            prompt_text += \"IMAGES are also provided (as base64). Analyze them if relevant.\\n\\n\"\n",
    "\n",
    "        prompt_text += \"\"\"\n",
    "        YOUR TASK:\n",
    "        1. Provide a well-structured summary (max ~500 words).\n",
    "        2. Capture the main ideas, trends, and insights across text, tables, and images.\n",
    "        3. If data is numeric (from tables), highlight key figures and patterns.\n",
    "        4. If images are included, mention what they add to the context.\n",
    "        5. End with a \"SEARCHABLE DESCRIPTION\" – 3-5 keywords or short phrases someone might use to find this content.\n",
    "\n",
    "        OUTPUT FORMAT:\n",
    "        ---------------\n",
    "        Summary:\n",
    "        [write summary here]\n",
    "\n",
    "        Searchable Description:\n",
    "        [keywords/phrases here]\n",
    "        \"\"\"\n",
    "\n",
    "        message_content= [{\"type\":\"text\", \"text\": prompt_text}]\n",
    "\n",
    "        for image_base64 in images:\n",
    "            message_content.append({\n",
    "                \"type\":\"image_url\",\n",
    "                \"image_url\": {\"url\":f\"data:image/jpeg;base64,{image_base64}\"}\n",
    "            })\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "        print(response.content)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        print(f\" AI summary failed : {e}\")\n",
    "        summary = f\"{text[:300]}...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "7cf0392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunks(chunks):\n",
    "    langchain_documents = []\n",
    "    total_chunks=len(chunks)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        current_chunk=i+1\n",
    "        print(f\"Processing chunk {current_chunk}/{total_chunks}\")\n",
    "\n",
    "        content_data = separate_content_types(chunk)\n",
    "\n",
    "        print(f\"Types found: {content_data['types']}\")\n",
    "        \n",
    "        if content_data[\"tables\"] or content_data['images']:\n",
    "            print(\"Creating AI summary for this\")\n",
    "            try: \n",
    "                enhanced_content = created_ai_summary(\n",
    "                    content_data['text'],\n",
    "                    content_data['tables'],\n",
    "                    content_data['images']\n",
    "                )\n",
    "                print(f\" AI summary created successfully balle balle\")\n",
    "                print(f\" Enhanced content preview {enhanced_content[:200]}...\")\n",
    "            except Exception as e:\n",
    "                print(\"AI summary failed sed sed\") \n",
    "                enhanced_content = content_data['text']\n",
    "        else:\n",
    "            print(\"Using raw text\") \n",
    "            enhanced_content = content_data['text']\n",
    "        \n",
    "        doc = Document(\n",
    "            page_content= enhanced_content,\n",
    "            metadata= {\n",
    "                \"original content\": json.dumps({\n",
    "                    \"raw_text\":content_data['text'],\n",
    "                    \"tables_html\": content_data[\"tables\"],\n",
    "                    \"images_base64\": content_data['images']\n",
    "\n",
    "                })\n",
    "            }\n",
    "        )\n",
    "            \n",
    "        langchain_documents.append(doc)\n",
    "    print(f\"Processed {len(langchain_documents)} chunks\")\n",
    "    return langchain_documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9dc4ec37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[4].metadata.orig_elements[7].metadata.image_base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7d871b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table><tbody><tr><td>publicly available, we cannot report test error rates for all the models that</td><td>+ [24] CNN</td><td>37.5%</td><td>17.0%</td></tr><tr><td>we tried. In the remainder of this paragraph, we use validation and test error rates interchangeably because</td><td>Table 1: Comparison</td><td>of results on</td><td>ILSVRC-</td></tr><tr><td>in our experience they do not differ by more than 0.1%</td><td>2010 test set. In</td><td>italics are</td><td>best results</td></tr></tbody></table>'"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[13].metadata.orig_elements[4].metadata.text_as_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fbc3b411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 2/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 3/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 4/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 5/19\n",
      "Types found: ['text', 'image']\n",
      "Creating AI summary for this\n",
      "Summary:\n",
      "\n",
      "The architecture of the discussed neural network consists of eight learned layers, including five convolutional and three fully-connected layers. A significant feature of this architecture is the use of Rectified Linear Units (ReLUs) as the nonlinearity function for neurons. Unlike traditional saturating nonlinearities such as tanh or sigmoid functions, ReLUs are non-saturating and defined as \\( f(x) = \\max(0, x) \\). This characteristic allows for much faster training times when using gradient descent, as demonstrated in the provided graph.\n",
      "\n",
      "The graph illustrates the training error rate over epochs for a four-layer convolutional neural network using ReLUs compared to one using tanh neurons. The network with ReLUs reaches a 25% training error rate on the CIFAR-10 dataset six times faster than the network with tanh neurons. This speed in learning is crucial for training large models on extensive datasets, as it significantly reduces the time required to achieve desired performance levels.\n",
      "\n",
      "The text also references previous work by Jarrett et al., who explored alternative neuron models, such as using the nonlinearity \\( f(x) = |\\tanh(x)| \\) in combination with contrast normalization and local average pooling. However, their focus was on preventing overfitting rather than accelerating training, which is the primary advantage of using ReLUs as highlighted in this study.\n",
      "\n",
      "Overall, the use of ReLUs in deep convolutional neural networks is emphasized as a key innovation that enhances training efficiency, making it feasible to experiment with larger networks and datasets.\n",
      "\n",
      "Searchable Description:\n",
      "ReLU nonlinearity, neural network architecture, fast training, CIFAR-10 dataset, convolutional layers\n",
      " AI summary created successfully balle balle\n",
      " Enhanced content preview Summary:\n",
      "\n",
      "The architecture of the discussed neural network consists of eight learned layers, including five convolutional and three fully-connected layers. A significant feature of this architecture i...\n",
      "Processing chunk 6/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 7/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 8/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 9/19\n",
      "Types found: ['text', 'image']\n",
      "Creating AI summary for this\n",
      "Summary:\n",
      "\n",
      "The architecture of the Convolutional Neural Network (CNN) described consists of eight layers with weights, divided into five convolutional layers and three fully-connected layers. The network is designed to classify images into 1000 categories using a softmax function at the output layer. The architecture aims to maximize the multinomial logistic regression objective, which involves optimizing the log-probability of the correct label across training cases.\n",
      "\n",
      "The CNN is implemented using two GPUs, with specific layers assigned to each GPU to optimize performance. The first convolutional layer processes a 224×224×3 input image using 96 kernels of size 11×11×3 with a stride of 4 pixels. The second layer uses 256 kernels of size 5×5×48, and the third layer employs 384 kernels of size 3×3×256. The fourth and fifth layers have 384 and 256 kernels of size 3×3×192, respectively. The fully-connected layers each contain 4096 neurons.\n",
      "\n",
      "Response-normalization layers are applied after the first and second convolutional layers, followed by max-pooling layers. The ReLU non-linearity is used after every convolutional and fully-connected layer to introduce non-linearity into the model. The architecture is designed to efficiently utilize the computational power of GPUs by connecting kernels in specific layers only to those on the same GPU, except for the third convolutional layer, which connects to all kernel maps in the second layer.\n",
      "\n",
      "The accompanying image illustrates the architecture, showing the flow of data through the layers and the division of tasks between the two GPUs. It visually represents the size and connections of each layer, providing a clear understanding of the network's structure.\n",
      "\n",
      "Searchable Description:\n",
      "CNN architecture, convolutional layers, fully-connected layers, GPU optimization, image classification\n",
      " AI summary created successfully balle balle\n",
      " Enhanced content preview Summary:\n",
      "\n",
      "The architecture of the Convolutional Neural Network (CNN) described consists of eight layers with weights, divided into five convolutional layers and three fully-connected layers. The netwo...\n",
      "Processing chunk 10/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 11/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 12/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 13/19\n",
      "Types found: ['text', 'image']\n",
      "Creating AI summary for this\n",
      "Summary:\n",
      "\n",
      "The content describes the training process of a neural network model using stochastic gradient descent. The training utilized a batch size of 128, a momentum of 0.9, and a weight decay of 0.0005. The weight decay was crucial not just as a regularizer but also in reducing the model's training error. The update rule for the weight \\( w \\) involved momentum and learning rate adjustments based on the derivative of the objective function.\n",
      "\n",
      "Weights were initialized from a zero-mean Gaussian distribution with a standard deviation of 0.01. Neuron biases in specific layers were set to 1 to accelerate learning by providing positive inputs to ReLUs, while biases in other layers were initialized to 0. The learning rate was uniformly applied across all layers and manually adjusted during training, starting at 0.01 and reduced by a factor of 10 when the validation error rate plateaued. The network underwent approximately 90 training cycles over a dataset of 1.2 million images, taking five to six days on two NVIDIA GTX 580 3GB GPUs.\n",
      "\n",
      "The accompanying image shows 96 convolutional kernels of size 11×11×3 learned by the first convolutional layer from 224×224×3 input images. The top 48 kernels were learned on GPU 1, and the bottom 48 on GPU 2, illustrating the distributed learning process across GPUs.\n",
      "\n",
      "Searchable Description:\n",
      "Neural network training, stochastic gradient descent, weight decay, convolutional kernels, GPU learning\n",
      " AI summary created successfully balle balle\n",
      " Enhanced content preview Summary:\n",
      "\n",
      "The content describes the training process of a neural network model using stochastic gradient descent. The training utilized a batch size of 128, a momentum of 0.9, and a weight decay of 0....\n",
      "Processing chunk 14/19\n",
      "Types found: ['table', 'text']\n",
      "Creating AI summary for this\n",
      "Summary:\n",
      "The document presents a detailed analysis of the performance of a Convolutional Neural Network (CNN) model on various ImageNet Large Scale Visual Recognition Challenge (ILSVRC) datasets, specifically focusing on the 2010 and 2012 competitions. The CNN model achieved significant improvements in error rates compared to previous methods.\n",
      "\n",
      "For the ILSVRC-2010 dataset, the CNN model achieved a top-1 error rate of 37.5% and a top-5 error rate of 17.0%. This performance surpassed the best results from the competition, which were 47.1% and 28.2% for top-1 and top-5 error rates, respectively, using a method that averaged predictions from six sparse-coding models. The best published results post-competition were 45.7% and 25.7% using Fisher Vectors (FVs) from densely-sampled features.\n",
      "\n",
      "In the ILSVRC-2012 competition, the CNN model continued to demonstrate superior performance. Although the test set labels for ILSVRC-2012 are not publicly available, the document reports a top-5 error rate of 18.2% for a single CNN. By averaging predictions from five similar CNNs, the error rate was reduced to 16.4%. Further improvements were achieved by training a CNN with an additional convolutional layer on the entire ImageNet Fall 2011 dataset and fine-tuning it on ILSVRC-2012, resulting in a 16.6% error rate. Combining predictions from two pre-trained CNNs with the five CNNs mentioned earlier yielded an even lower error rate of 15.3%. The second-best entry in the competition had a top-5 error rate of 26.2%, achieved by averaging predictions from classifiers trained on FVs from various densely-sampled features.\n",
      "\n",
      "Additionally, the document reports results on the Fall 2009 version of ImageNet, which contains 10,184 categories and 8.9 million images. The CNN model achieved top-1 and top-5 error rates of 67.4% on this dataset, following a conventional split of using half the images for training and half for testing.\n",
      "\n",
      "The tables included in the document provide a comparison of error rates across different models and datasets, highlighting the superior performance of the CNN model. Although no images are provided, the text and tables effectively convey the improvements in error rates achieved by the CNN model.\n",
      "\n",
      "Searchable Description:\n",
      "CNN performance, ILSVRC-2010, ILSVRC-2012, ImageNet error rates, deep learning advancements\n",
      " AI summary created successfully balle balle\n",
      " Enhanced content preview Summary:\n",
      "The document presents a detailed analysis of the performance of a Convolutional Neural Network (CNN) model on various ImageNet Large Scale Visual Recognition Challenge (ILSVRC) datasets, spec...\n",
      "Processing chunk 15/19\n",
      "Types found: ['table', 'text']\n",
      "Creating AI summary for this\n",
      "Summary:\n",
      "The content provided focuses on the performance comparison of different models on the ILSVRC-2012 validation and test sets, specifically highlighting error rates. The models compared include SIFT + FVs, 1 CNN, 5 CNNs, 1 CNN*, and 7 CNNs*. The asterisk (*) denotes models that were pre-trained to classify the entire ImageNet 2011 Fall release, which is a significant detail as it implies these models had additional training data.\n",
      "\n",
      "The table provided (Table 1) outlines the performance of these models in terms of Top-1 and Top-5 error rates on both validation and test sets. The Top-1 error rate refers to the percentage of test images for which the correct label is not the top predicted label, while the Top-5 error rate refers to the percentage of test images for which the correct label is not within the top five predicted labels.\n",
      "\n",
      "Key figures from the table include:\n",
      "- The SIFT + FVs model achieved a Top-5 test error rate of 26.2%.\n",
      "- The 1 CNN model achieved a Top-1 validation error rate of 40.7% and a Top-5 validation error rate of 18.2%.\n",
      "- The 5 CNNs model had a Top-5 test error rate of 16.4%.\n",
      "- The 1 CNN* model, which was pre-trained, achieved a Top-1 validation error rate of 39.0% and a Top-5 validation error rate of 16.6%.\n",
      "- The 7 CNNs* model, also pre-trained, achieved the lowest Top-5 test error rate of 15.3%.\n",
      "\n",
      "The text mentions that the best published results on this dataset are 78.1% and 60.9%, although it is not clear from the provided content what these percentages specifically refer to (likely accuracy rates).\n",
      "\n",
      "The additional text notes that a model with an additional sixth convolutional layer over the last pooling layer achieved a 40.9% error rate, although it is not specified whether this is a Top-1 or Top-5 error rate.\n",
      "\n",
      "Overall, the content highlights the trend that models with more convolutional layers and pre-training on larger datasets tend to perform better, as evidenced by the lower error rates of the 7 CNNs* model. This suggests that both the depth of the network and the amount of pre-training data are crucial factors in improving model performance on image classification tasks.\n",
      "\n",
      "Searchable Description:\n",
      "ILSVRC-2012, CNN performance, image classification error rates, pre-trained models, convolutional layers.\n",
      " AI summary created successfully balle balle\n",
      " Enhanced content preview Summary:\n",
      "The content provided focuses on the performance comparison of different models on the ILSVRC-2012 validation and test sets, specifically highlighting error rates. The models compared include ...\n",
      "Processing chunk 16/19\n",
      "Types found: ['text', 'image']\n",
      "Creating AI summary for this\n",
      "Summary:\n",
      "\n",
      "The document presents a qualitative evaluation of a neural network's performance, focusing on its ability to learn and recognize visual patterns. In Figure 3, the convolutional kernels learned by the network's two data-connected layers are shown. These kernels exhibit specialization, with GPU 1 learning color-agnostic kernels and GPU 2 learning color-specific ones. This specialization is consistent across different runs and is not affected by random weight initialization.\n",
      "\n",
      "The text also discusses error rates, noting that without averaging predictions over ten patches, the error rates are 39.0% and 18.3%. Figure 4 provides a qualitative assessment of the network's learning by showing its top-5 predictions on eight test images. The network can recognize off-center objects, and most top-5 labels are reasonable, though some images present genuine ambiguity.\n",
      "\n",
      "The network's visual knowledge is further explored by examining feature activations in the last hidden layer. Images with similar feature activation vectors are considered similar by the network, even if they are not pixel-wise similar. This is demonstrated in Figure 4, where test images are compared with training images that have the smallest Euclidean distance in feature space. The retrieved images, such as dogs and elephants, vary in pose, indicating the network's ability to generalize beyond pixel-level similarities.\n",
      "\n",
      "The images provided illustrate these concepts. The first image shows test images with their top-5 predicted labels, highlighting the network's ability to make reasonable predictions. The second image demonstrates the network's ability to find similar images in feature space, despite differences in pixel-level appearance.\n",
      "\n",
      "Searchable Description:\n",
      "Neural network evaluation, convolutional kernels, image recognition, feature activation, visual learning.\n",
      " AI summary created successfully balle balle\n",
      " Enhanced content preview Summary:\n",
      "\n",
      "The document presents a qualitative evaluation of a neural network's performance, focusing on its ability to learn and recognize visual patterns. In Figure 3, the convolutional kernels learn...\n",
      "Processing chunk 17/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 18/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 19/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processed 19 chunks\n"
     ]
    }
   ],
   "source": [
    "docs1 =summarize_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9f4c209b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(json.loads(docs1[0].metadata[\"original content\"])[\"tables_html\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "86b0fc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'original content': '{\"raw_text\": \"ImageNet Classi\\\\ufb01cation with Deep Convolutional Neural Networks\\\\n\\\\nAlex Krizhevsky\\\\n\\\\nUniversity of Toronto kriz@cs.utoronto.ca\\\\n\\\\nIlya Sutskever University of Toronto ilya@cs.utoronto.ca\\\\n\\\\nGeoffrey E. Hinton\\\\n\\\\nUniversity of Toronto hinton@cs.utoronto.ca\\\\n\\\\nAbstract\\\\n\\\\nWe trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif- ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of \\\\ufb01ve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a \\\\ufb01nal 1000-way softmax. To make train- ing faster, we used non-saturating neurons and a very ef\\\\ufb01cient GPU implemen- tation of the convolution operation. To reduce over\\\\ufb01tting in the fully-connected layers we employed a recently-developed regularization method called \\\\u201cdropout\\\\u201d that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.\", \"tables_html\": [], \"images_base64\": []}'}, page_content='ImageNet Classiﬁcation with Deep Convolutional Neural Networks\\n\\nAlex Krizhevsky\\n\\nUniversity of Toronto kriz@cs.utoronto.ca\\n\\nIlya Sutskever University of Toronto ilya@cs.utoronto.ca\\n\\nGeoffrey E. Hinton\\n\\nUniversity of Toronto hinton@cs.utoronto.ca\\n\\nAbstract\\n\\nWe trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif- ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of ﬁve convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a ﬁnal 1000-way softmax. To make train- ing faster, we used non-saturating neurons and a very efﬁcient GPU implemen- tation of the convolution operation. To reduce overﬁtting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.')"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3bf6d9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summary:\\nThe content provided focuses on the performance comparison of different models on the ILSVRC-2012 validation and test sets, specifically highlighting error rates. The models compared include SIFT + FVs, 1 CNN, 5 CNNs, 1 CNN*, and 7 CNNs*. The asterisk (*) denotes models that were pre-trained to classify the entire ImageNet 2011 Fall release, which is a significant detail as it implies these models had additional training data.\\n\\nThe table provided (Table 1) outlines the performance of these models in terms of Top-1 and Top-5 error rates on both validation and test sets. The Top-1 error rate refers to the percentage of test images for which the correct label is not the top predicted label, while the Top-5 error rate refers to the percentage of test images for which the correct label is not within the top five predicted labels.\\n\\nKey figures from the table include:\\n- The SIFT + FVs model achieved a Top-5 test error rate of 26.2%.\\n- The 1 CNN model achieved a Top-1 validation error rate of 40.7% and a Top-5 validation error rate of 18.2%.\\n- The 5 CNNs model had a Top-5 test error rate of 16.4%.\\n- The 1 CNN* model, which was pre-trained, achieved a Top-1 validation error rate of 39.0% and a Top-5 validation error rate of 16.6%.\\n- The 7 CNNs* model, also pre-trained, achieved the lowest Top-5 test error rate of 15.3%.\\n\\nThe text mentions that the best published results on this dataset are 78.1% and 60.9%, although it is not clear from the provided content what these percentages specifically refer to (likely accuracy rates).\\n\\nThe additional text notes that a model with an additional sixth convolutional layer over the last pooling layer achieved a 40.9% error rate, although it is not specified whether this is a Top-1 or Top-5 error rate.\\n\\nOverall, the content highlights the trend that models with more convolutional layers and pre-training on larger datasets tend to perform better, as evidenced by the lower error rates of the 7 CNNs* model. This suggests that both the depth of the network and the amount of pre-training data are crucial factors in improving model performance on image classification tasks.\\n\\nSearchable Description:\\nILSVRC-2012, CNN performance, image classification error rates, pre-trained models, convolutional layers.'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs1[14].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4ce7c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(docs, persist_directory=\"db2/chroma_db\"):\n",
    "    \"\"\" Create and persist ChromaDB vector store\"\"\" \n",
    "    print(\"Creating embeddings and storing in ChromaDB...\")\n",
    "    embedding_model = OpenAIEmbeddings(model= \"text-embedding-3-small\")\n",
    "\n",
    "    print(\"--Creating vector store ---\")\n",
    "    vectorstore = Chroma.from_documents (\n",
    "        documents= docs,\n",
    "        embedding= embedding_model,\n",
    "        persist_directory=persist_directory,\n",
    "        collection_metadata = {\"hnsw:space\" : \"cosine\"}\n",
    "    )\n",
    "\n",
    "    print(\"Finished Creating vector store\")\n",
    "    print(f\"vector store creted and stored to {persist_directory}\")\n",
    "    return vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0741d416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings and storing in ChromaDB...\n",
      "--Creating vector store ---\n",
      "Finished Creating vector store\n",
      "vector store creted and stored to db2/chroma_db\n"
     ]
    }
   ],
   "source": [
    "db = create_vector_store(docs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "69c9487e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_answer(chunks,query):\n",
    "    \"\"\" generate final answer to the query \"\"\"\n",
    "    try: \n",
    "        llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "        prompt_text = f\"\"\"You are an expert assistant. Your job is to read the provided documents \n",
    "        (text, tables, and images if available) and give a clear, accurate, and well-structured \n",
    "        answer to the user query.\n",
    "\n",
    "        USER QUERY:\n",
    "        {query}\n",
    "\n",
    "        CONTENT TO ANALYZE:\"\"\" \n",
    "        for i,chunk in enumerate(chunks):\n",
    "            \n",
    "            prompt_text += f\"---Document{i+1} --- \\n\"\n",
    "            raw_text=json.loads(chunks[i].metadata[\"original content\"])[\"raw_text\"]\n",
    "            #print(f\"CHUNK NO:{i}\\n RAW TEXT:{raw_text}\")\n",
    "            prompt_text+= f\"TEXT:\\n{raw_text}\\n\\n\"\n",
    "            \n",
    "            #print(json.loads(chunks[i].metadata[\"original_content\"])[\"tables_html\"])\n",
    "            #original_data = json.loads(chunks[i].metadata[\"original_content\"])[\"tables_html\"]\n",
    "            \n",
    "            if json.loads(chunks[i].metadata[\"original content\"])[\"tables_html\"]:\n",
    "                prompt_text += \"TABLES: \\n\"\n",
    "                for j,table in enumerate(json.loads(chunks[i].metadata[\"original content\"])[\"tables_html\"]):\n",
    "                    prompt_text +=f\"Tables {j+1}: \\n {table} \\n \\n\"\n",
    "            prompt_text += \"\\n\"\n",
    "            prompt_text += \"\"\"INSTRUCTIONS:\n",
    "            1. Use the documents as your main source of truth.\n",
    "            2. If images are provided, describe what they show and connect it with the text.\n",
    "            3. Summarize and explain clearly, avoid copying raw text.\n",
    "            4. Be concise but thorough.\n",
    "            5. If data is not provided in the retrieved documents the just say that I dont have enough information\n",
    "            \n",
    "            FINAL ANSWER:\"\"\" \n",
    "            \n",
    "        message_content = [{\"type\":\"text\",\"text\": prompt_text}]\n",
    "        for chunk in chunks:\n",
    "            if \"original content\"in chunk.metadata:\n",
    "                original_data= json.loads(chunk.metadata[\"original content\"])\n",
    "                images_base64 = original_data.get(\"images_base64\", [])\n",
    "                for images_base64 in images_base64:\n",
    "                    message_content.append({\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\" : {\"url\":  f\"data:image/jpeg;base64,{images_base64}\"}\n",
    "                    })\n",
    "        print(message_content)\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "        return response.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"answer generation failed \")\n",
    "        return \"Problem occured, retry\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "776a7639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<unstructured.documents.elements.CompositeElement at 0x7fc81b406210>"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "77a93a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------STARTING THE PIPELINE------------------\n",
      "Warning: No languages specified, defaulting to English.\n",
      "Extracted 137 elements\n",
      "------------------CHUNKING BY TITLE------------------\n",
      "Created 19 chunks\n",
      "------------------CREATING SUMMARY FOR MULTIMODAL DATA------------------\n",
      "Processing chunk 1/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 2/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 3/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 4/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 5/19\n",
      "Types found: ['text', 'image']\n",
      "Creating AI summary for this\n",
      "Summary:\n",
      "\n",
      "The architecture of the discussed neural network consists of eight learned layers, including five convolutional and three fully-connected layers. A significant feature of this architecture is the use of Rectified Linear Units (ReLUs) as the nonlinearity function for neurons. Unlike traditional saturating nonlinearities such as tanh or sigmoid functions, ReLUs are non-saturating and allow for much faster training times when using gradient descent. This is particularly beneficial for deep convolutional neural networks, enabling them to train several times faster than networks using traditional neuron models.\n",
      "\n",
      "The text references a study by Nair and Hinton, which supports the use of ReLUs for faster training. The document also contrasts this approach with other alternatives, such as the nonlinearity f(x) = |tanh(x)| used by Jarrett et al., which focuses on preventing overfitting rather than accelerating training.\n",
      "\n",
      "The accompanying image illustrates the performance difference between networks using ReLUs and those using tanh neurons. The graph shows a four-layer convolutional neural network with ReLUs reaching a 25% training error rate on the CIFAR-10 dataset six times faster than an equivalent network with tanh neurons. This demonstrates the significant impact of ReLUs on training efficiency, especially for large models and datasets.\n",
      "\n",
      "Overall, the use of ReLUs in neural network architecture is highlighted as a crucial factor in improving training speed and efficiency, making it possible to experiment with larger networks.\n",
      "\n",
      "Searchable Description:\n",
      "ReLU nonlinearity, neural network architecture, fast training, convolutional layers, CIFAR-10 dataset.\n",
      " AI summary created successfully balle balle\n",
      " Enhanced content preview Summary:\n",
      "\n",
      "The architecture of the discussed neural network consists of eight learned layers, including five convolutional and three fully-connected layers. A significant feature of this architecture i...\n",
      "Processing chunk 6/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 7/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 8/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 9/19\n",
      "Types found: ['text', 'image']\n",
      "Creating AI summary for this\n",
      "Summary:\n",
      "\n",
      "The described Convolutional Neural Network (CNN) architecture consists of eight layers with weights, divided into five convolutional layers and three fully-connected layers. The network is designed to classify images into 1000 categories using a softmax function at the output layer. The architecture aims to maximize the multinomial logistic regression objective, enhancing the log-probability of correct label predictions.\n",
      "\n",
      "The CNN is implemented across two GPUs, with specific layers assigned to each GPU to optimize processing. The first convolutional layer processes a 224×224×3 input image using 96 kernels of size 11×11×3 with a stride of 4 pixels. Subsequent layers increase in complexity, with the second layer using 256 kernels of size 5×5×48, and the third, fourth, and fifth layers using 384 and 256 kernels of size 3×3 with varying depths. The fully-connected layers each contain 4096 neurons.\n",
      "\n",
      "Response-normalization layers follow the first and second convolutional layers, and max-pooling layers are applied after these normalization layers and the fifth convolutional layer. The ReLU activation function is used throughout the network to introduce non-linearity.\n",
      "\n",
      "The accompanying image illustrates the architecture, showing the flow of data through the layers and the division of tasks between the two GPUs. The image provides a visual representation of the layer connections, kernel sizes, and pooling operations, enhancing the understanding of the network's structure.\n",
      "\n",
      "Searchable Description:\n",
      "CNN architecture, convolutional layers, fully-connected layers, GPU processing, image classification\n",
      " AI summary created successfully balle balle\n",
      " Enhanced content preview Summary:\n",
      "\n",
      "The described Convolutional Neural Network (CNN) architecture consists of eight layers with weights, divided into five convolutional layers and three fully-connected layers. The network is d...\n",
      "Processing chunk 10/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 11/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 12/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 13/19\n",
      "Types found: ['text', 'image']\n",
      "Creating AI summary for this\n",
      "Summary:\n",
      "\n",
      "The content describes the training process of a neural network model using stochastic gradient descent. The training utilized a batch size of 128, a momentum of 0.9, and a weight decay of 0.0005. The weight decay was crucial not just as a regularizer but also in reducing the model's training error. The update rule for the weight \\( w \\) involved momentum and learning rate adjustments based on the derivative of the objective function.\n",
      "\n",
      "Weights were initialized from a zero-mean Gaussian distribution with a standard deviation of 0.01. Neuron biases in specific layers were initialized with the constant 1 to accelerate learning by providing positive inputs to ReLUs, while other layers had biases initialized to 0. The learning rate was uniformly applied across all layers and manually adjusted during training, starting at 0.01 and reduced by a factor of 10 when the validation error rate plateaued. The network was trained over approximately 90 cycles on a dataset of 1.2 million images, taking five to six days on two NVIDIA GTX 580 3GB GPUs.\n",
      "\n",
      "The accompanying image shows 96 convolutional kernels of size 11×11×3 learned by the first convolutional layer from 224×224×3 input images. The top 48 kernels were learned on GPU 1, and the bottom 48 on GPU 2, illustrating the distributed learning process across GPUs.\n",
      "\n",
      "Searchable Description:\n",
      "Neural network training, stochastic gradient descent, convolutional kernels, weight decay, GPU learning\n",
      " AI summary created successfully balle balle\n",
      " Enhanced content preview Summary:\n",
      "\n",
      "The content describes the training process of a neural network model using stochastic gradient descent. The training utilized a batch size of 128, a momentum of 0.9, and a weight decay of 0....\n",
      "Processing chunk 14/19\n",
      "Types found: ['table', 'text']\n",
      "Creating AI summary for this\n",
      "Summary:\n",
      "The document presents a detailed analysis of the performance of a Convolutional Neural Network (CNN) model on various ImageNet Large Scale Visual Recognition Challenge (ILSVRC) datasets, specifically focusing on the 2010 and 2012 competitions. The CNN model achieved significant improvements in error rates compared to previous methods.\n",
      "\n",
      "For the ILSVRC-2010 dataset, the CNN model achieved a top-1 error rate of 37.5% and a top-5 error rate of 17.0%. This performance surpassed the best results from the competition, which were 47.1% and 28.2% for top-1 and top-5 error rates, respectively, using a method that averaged predictions from six sparse-coding models. The best published results post-competition were 45.7% and 25.7% using Fisher Vectors (FVs) from densely-sampled features.\n",
      "\n",
      "In the ILSVRC-2012 competition, the CNN model continued to demonstrate superior performance. Although the test set labels for ILSVRC-2012 are not publicly available, the document reports a top-5 error rate of 18.2% for a single CNN. By averaging predictions from five similar CNNs, the error rate was reduced to 16.4%. Further improvements were achieved by training a CNN with an additional convolutional layer on the entire ImageNet Fall 2011 dataset and fine-tuning it on ILSVRC-2012, resulting in a 16.6% error rate. Combining predictions from two pre-trained CNNs with the five aforementioned CNNs further reduced the error rate to 15.3%. The second-best entry in the competition had a top-5 error rate of 26.2%, using a method that averaged predictions from several classifiers trained on FVs.\n",
      "\n",
      "Additionally, the document reports results on the Fall 2009 version of ImageNet, which contains 10,184 categories and 8.9 million images. The CNN model achieved top-1 and top-5 error rates of 67.4% on this dataset, following a conventional split of using half the images for training and half for testing.\n",
      "\n",
      "The tables included in the document provide a comparison of the CNN model's performance against other methods, highlighting the significant reduction in error rates achieved by the CNN approach. Although no images are provided, the text and tables effectively convey the improvements and trends in CNN performance over traditional methods.\n",
      "\n",
      "Searchable Description:\n",
      "CNN performance, ILSVRC-2010, ILSVRC-2012, ImageNet error rates, deep learning advancements\n",
      " AI summary created successfully balle balle\n",
      " Enhanced content preview Summary:\n",
      "The document presents a detailed analysis of the performance of a Convolutional Neural Network (CNN) model on various ImageNet Large Scale Visual Recognition Challenge (ILSVRC) datasets, spec...\n",
      "Processing chunk 15/19\n",
      "Types found: ['table', 'text']\n",
      "Creating AI summary for this\n",
      "Summary:\n",
      "The content provided focuses on the performance comparison of different models on the ILSVRC-2012 validation and test sets, specifically highlighting error rates. The models compared include SIFT + FVs, 1 CNN, 5 CNNs, 1 CNN*, and 7 CNNs*. The asterisk (*) denotes models that were pre-trained to classify the entire ImageNet 2011 Fall release, which is a significant detail as it implies these models had additional training data that could influence their performance.\n",
      "\n",
      "The table provided (Table 1) presents the Top-1 and Top-5 error rates for both validation and test sets. The SIFT + FVs model shows a Top-5 test error rate of 26.2%, which is notably higher than the CNN-based models. The 1 CNN model achieves a Top-1 validation error rate of 40.7% and a Top-5 validation error rate of 18.2%. The 5 CNNs model shows a slight improvement with a Top-5 validation and test error rate of 16.4%. The pre-trained models, 1 CNN* and 7 CNNs*, show further improvements, with the 7 CNNs* model achieving the lowest Top-5 test error rate of 15.3%.\n",
      "\n",
      "The text mentions that the best results on this dataset, achieved by others, are 78.1% and 60.9%, indicating that the models discussed are not the top performers but are competitive. Additionally, a model with an additional sixth convolutional layer over the last pooling layer achieved a 40.9% error rate, suggesting that increasing the depth of the network can potentially improve performance.\n",
      "\n",
      "Overall, the content highlights the trend that increasing the number of CNNs and pre-training on a larger dataset can lead to better performance in terms of lower error rates. The comparison underscores the importance of model architecture and training data in achieving competitive results in image classification tasks.\n",
      "\n",
      "Searchable Description:\n",
      "CNN performance, ILSVRC-2012 error rates, ImageNet pre-training, model comparison, deep learning models.\n",
      " AI summary created successfully balle balle\n",
      " Enhanced content preview Summary:\n",
      "The content provided focuses on the performance comparison of different models on the ILSVRC-2012 validation and test sets, specifically highlighting error rates. The models compared include ...\n",
      "Processing chunk 16/19\n",
      "Types found: ['text', 'image']\n",
      "Creating AI summary for this\n",
      "Summary:\n",
      "\n",
      "The content discusses the qualitative evaluations of a neural network's performance, focusing on its ability to learn and recognize visual patterns. Figure 3 illustrates the convolutional kernels learned by the network's two data-connected layers, highlighting the specialization between two GPUs. GPU 1 primarily learns color-agnostic kernels, while GPU 2 focuses on color-specific kernels. This specialization is consistent across different runs and is not influenced by random weight initialization.\n",
      "\n",
      "The text also examines the network's top-5 predictions on eight test images from the ILSVRC-2010 dataset, as shown in the left panel of Figure 4. The network demonstrates the ability to recognize off-center objects, such as a mite, and provides reasonable top-5 labels for various images. For instance, only other types of cats are considered plausible labels for a leopard. However, some images, like a grille or cherry, present genuine ambiguity regarding the photograph's focus.\n",
      "\n",
      "Another method to evaluate the network's visual knowledge is by analyzing feature activations in the last hidden layer. Images with similar feature activation vectors are considered similar by the network, even if they differ at the pixel level. The right panel of Figure 4 displays five test images alongside the six most similar training images based on Euclidean distance in feature space. This comparison reveals that the network can recognize similarities in images despite variations in poses or appearances, such as with dogs and elephants.\n",
      "\n",
      "The images provided further illustrate these findings. The first image shows the network's top-5 predictions for various objects, while the second image demonstrates the network's ability to find similar images in the training set based on feature activations. These visualizations underscore the network's capacity to learn complex visual patterns and make accurate predictions.\n",
      "\n",
      "Searchable Description:\n",
      "Neural network evaluation, convolutional kernels, image recognition, feature activations, ILSVRC-2010 dataset.\n",
      " AI summary created successfully balle balle\n",
      " Enhanced content preview Summary:\n",
      "\n",
      "The content discusses the qualitative evaluations of a neural network's performance, focusing on its ability to learn and recognize visual patterns. Figure 3 illustrates the convolutional ke...\n",
      "Processing chunk 17/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 18/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processing chunk 19/19\n",
      "Types found: ['text']\n",
      "Using raw text\n",
      "Processed 19 chunks\n",
      "------------------CREATING VECTOR STORE------------------\n",
      "Creating embeddings and storing in ChromaDB...\n",
      "--Creating vector store ---\n",
      "Finished Creating vector store\n",
      "vector store creted and stored to db2/chroma_db\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------STARTING THE PIPELINE------------------\")\n",
    "file_path = \"imagenet.pdf\"\n",
    "elements=partition_document(file_path)\n",
    "print(\"------------------CHUNKING BY TITLE------------------\")\n",
    "chunks=create_chunks_by_title(elements)\n",
    "print(\"------------------CREATING SUMMARY FOR MULTIMODAL DATA------------------\")\n",
    "docs1=summarize_chunks(chunks)\n",
    "print(\"------------------CREATING VECTOR STORE------------------\")\n",
    "db = create_vector_store(docs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "1ebe9a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------RETRIEVAL DONE-------------------\n",
      "[{'type': 'text', 'text': 'You are an expert assistant. Your job is to read the provided documents \\n        (text, tables, and images if available) and give a clear, accurate, and well-structured \\n        answer to the user query.\\n\\n        USER QUERY:\\n        what does this model give results so much better than any other approaches\\n\\n        CONTENT TO ANALYZE:---Document1 --- \\nTEXT:\\nModel SIFT + FVs [7] 1 CNN 5 CNNs 1 CNN* 7 CNNs* Top-1 (val) Top-5 (val) Top-5 (test) — — 26.2% 40.7% 18.2% — 38.1% 16.4% 16.4% 39.0% 16.6% — 36.7% 15.4% 15.3%\\n\\nTable 2: Comparison of error rates on ILSVRC-2012 validation and test sets. In italics are best results achieved by others. Models with an asterisk* were “pre-trained” to classify the entire ImageNet 2011 Fall release. See Section 6 for details.\\n\\n40.9%, attained by the net described above but with an additional, sixth convolutional layer over the last pooling layer. The best published results on this dataset are 78.1% and 60.9% [19].\\n\\nTABLES: \\nTables 1: \\n <table><thead><tr><th>Model</th><th>Top-1 (val)</th><th>| Top-5 (val)</th><th>| Top-5 (test)</th></tr></thead><tbody><tr><td>SIFT + FVs</td><td>[7] —</td><td>=</td><td>26.2%</td></tr><tr><td>1 CNN</td><td>40.7%</td><td>18.2%</td><td>=</td></tr><tr><td>5 CNNs</td><td>38.1%</td><td>16.4%</td><td>16.4%</td></tr><tr><td>1 CNN*</td><td>39.0%</td><td>16.6%</td><td>=</td></tr><tr><td>7 CNNs*</td><td>36.7%</td><td>15.4%</td><td>15.3%</td></tr></tbody></table> \\n \\n\\nINSTRUCTIONS:\\n            1. Use the documents as your main source of truth.\\n            2. If images are provided, describe what they show and connect it with the text.\\n            3. Summarize and explain clearly, avoid copying raw text.\\n            4. Be concise but thorough.\\n            5. If data is not provided in the retrieved documents the just say that I dont have enough information\\n\\n            FINAL ANSWER:---Document2 --- \\nTEXT:\\n4.2 Dropout\\n\\nCombining the predictions of many different models is a very successful way to reduce test errors [1, 3], but it appears to be too expensive for big neural networks that already take several days to train. There is, however, a very efﬁcient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout” [10], consists of setting to zero the output of each hidden neuron with probability 0.5. The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in back- propagation. So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks.\\n\\nWe use dropout in the ﬁrst two fully-connected layers of Figure 2. Without dropout, our network ex- hibits substantial overﬁtting. Dropout roughly doubles the number of iterations required to converge.\\n\\n\\nINSTRUCTIONS:\\n            1. Use the documents as your main source of truth.\\n            2. If images are provided, describe what they show and connect it with the text.\\n            3. Summarize and explain clearly, avoid copying raw text.\\n            4. Be concise but thorough.\\n            5. If data is not provided in the retrieved documents the just say that I dont have enough information\\n\\n            FINAL ANSWER:---Document3 --- \\nTEXT:\\n4.2 Dropout\\n\\nCombining the predictions of many different models is a very successful way to reduce test errors [1, 3], but it appears to be too expensive for big neural networks that already take several days to train. There is, however, a very efﬁcient version of model combination that only costs about a factor of two during training. The recently-introduced technique, called “dropout” [10], consists of setting to zero the output of each hidden neuron with probability 0.5. The neurons which are “dropped out” in this way do not contribute to the forward pass and do not participate in back- propagation. So every time an input is presented, the neural network samples a different architecture, but all these architectures share weights. This technique reduces complex co-adaptations of neurons, since a neuron cannot rely on the presence of particular other neurons. It is, therefore, forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. At test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks.\\n\\nWe use dropout in the ﬁrst two fully-connected layers of Figure 2. Without dropout, our network ex- hibits substantial overﬁtting. Dropout roughly doubles the number of iterations required to converge.\\n\\n\\nINSTRUCTIONS:\\n            1. Use the documents as your main source of truth.\\n            2. If images are provided, describe what they show and connect it with the text.\\n            3. Summarize and explain clearly, avoid copying raw text.\\n            4. Be concise but thorough.\\n            5. If data is not provided in the retrieved documents the just say that I dont have enough information\\n\\n            FINAL ANSWER:'}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The model described in the documents achieves superior results compared to other approaches primarily due to the use of advanced techniques such as dropout and the architecture of the neural network.\\n\\n1. **Dropout Technique**: Dropout is a regularization method that helps prevent overfitting in neural networks. By randomly setting the output of each hidden neuron to zero with a probability of 0.5 during training, the network effectively samples different architectures. This forces the network to learn more robust features that are not reliant on specific neurons, thus improving generalization. At test time, all neurons are used, but their outputs are scaled down, which approximates the geometric mean of the predictions from the various dropout networks. This technique allows the model to reduce test errors efficiently without the computational cost of training multiple separate models.\\n\\n2. **Network Architecture**: The model's architecture, which includes multiple convolutional layers, contributes to its high performance. The document mentions a network with an additional sixth convolutional layer, which achieves a top-1 validation error rate of 40.9%. This architecture, combined with the dropout technique, allows the model to outperform others on the ILSVRC-2012 dataset.\\n\\nOverall, the combination of dropout and a well-designed network architecture enables the model to achieve better results than other approaches.\""
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what does this model give results so much better than any other approaches\" \n",
    "retriever = db.as_retriever(search_kwargs={\"k\":3})\n",
    "chunks = retriever.invoke(query)\n",
    "print(\"---------------------RETRIEVAL DONE-------------------\")\n",
    "generate_final_answer(chunks,query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
